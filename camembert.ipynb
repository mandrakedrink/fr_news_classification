{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# In the beginning let's install transformers library\n!pip install transformers","execution_count":1,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (3.5.1)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.43)\nRequirement already satisfied: sentencepiece==0.1.91 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.1.91)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers) (20.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.23.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2020.4.4)\nRequirement already satisfied: tokenizers==0.9.3 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.9.3)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.7/site-packages (from transformers) (3.14.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.45.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from transformers) (1.18.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.10)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (1.14.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (1.14.0)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.9)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2020.11.8)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.25.9)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2020.4.4)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.45.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.1)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (0.14.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (1.14.0)\n\u001b[33mWARNING: You are using pip version 20.3.1; however, version 20.3.3 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom tqdm import tqdm, trange\nimport numpy as np\n\n\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.sequence import pad_sequences\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import CamembertTokenizer, CamembertForSequenceClassification\nfrom transformers import AdamW\n# import torch.optim as optim\n# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n# import seaborn as sns","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining constants\nepochs = 3#0\nMAX_LEN = 512#256#128\nbatch_size = 16\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/sport-fr/eurosport_fr.csv\")\n\ndf['category'].replace({'formule-1':\"course\", 'auto-moto':\"course\", 'motocyclisme':\"course\"}, inplace=True)\ndf.category.value_counts()\n\ndf['category_id'] = df['category'].factorize()[0]\ndf = df.loc[df.text.notnull()].reset_index(drop=True)\nprint(len(df.category.value_counts()))\ndf.head()","execution_count":4,"outputs":[{"output_type":"stream","text":"18\n","name":"stdout"},{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"                                                 url  \\\n0  https://www.eurosport.fr//formule-1/michael-n-...   \n1  https://www.eurosport.fr//formule-1/saison-202...   \n2  https://www.eurosport.fr//formule-1/saison-202...   \n3  https://www.eurosport.fr//formule-1/formule-1-...   \n4  https://www.eurosport.fr//formule-1/frank-will...   \n\n                                               title  \\\n0  \"Michael n'était pas très fort pour livrer une...   \n1  Saison 2021 de Formule 1 : Le calendrier confi...   \n2  Transferts - AlphaTauri met fin au suspens : Y...   \n3  Formule 1 - Rebondissement de dernière minute ...   \n4                         Frank Williams hospitalisé   \n\n                                           sub_title  \\\n0  FORMULE 1 - Mattia Binotto fut l'une des rares...   \n1  SAISON 2021 - A l'occasion du Conseil Mondial ...   \n2  SAISON 2021 - Ce n'était un secret pour person...   \n3  FORMULE 1 - Selon les informations du quotidie...   \n4  L'écurie Williams a fait savoir que Sir Frank ...   \n\n                                             img_src  \\\n0  https://imgresizer.eurosport.com/unsafe/1200x0...   \n1  https://imgresizer.eurosport.com/unsafe/1200x0...   \n2  https://imgresizer.eurosport.com/unsafe/1200x0...   \n3  https://imgresizer.eurosport.com/unsafe/1200x0...   \n4  https://imgresizer.eurosport.com/unsafe/1200x0...   \n\n                                                text category  category_id  \n0  Jean Todt avait jeté son dévolu sur Michael Sc...   course            0  \n1  Si les conditions sanitaires le permettent, vi...   course            0  \n2  C’était un secret de Polichinelle, Red Bull l’...   course            0  \n3  Soirée mouvementée du côté du paddock. Alors q...   course            0  \n4  L'année 2020 n'est pas de tout repos pour la f...   course            0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>url</th>\n      <th>title</th>\n      <th>sub_title</th>\n      <th>img_src</th>\n      <th>text</th>\n      <th>category</th>\n      <th>category_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>https://www.eurosport.fr//formule-1/michael-n-...</td>\n      <td>\"Michael n'était pas très fort pour livrer une...</td>\n      <td>FORMULE 1 - Mattia Binotto fut l'une des rares...</td>\n      <td>https://imgresizer.eurosport.com/unsafe/1200x0...</td>\n      <td>Jean Todt avait jeté son dévolu sur Michael Sc...</td>\n      <td>course</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>https://www.eurosport.fr//formule-1/saison-202...</td>\n      <td>Saison 2021 de Formule 1 : Le calendrier confi...</td>\n      <td>SAISON 2021 - A l'occasion du Conseil Mondial ...</td>\n      <td>https://imgresizer.eurosport.com/unsafe/1200x0...</td>\n      <td>Si les conditions sanitaires le permettent, vi...</td>\n      <td>course</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>https://www.eurosport.fr//formule-1/saison-202...</td>\n      <td>Transferts - AlphaTauri met fin au suspens : Y...</td>\n      <td>SAISON 2021 - Ce n'était un secret pour person...</td>\n      <td>https://imgresizer.eurosport.com/unsafe/1200x0...</td>\n      <td>C’était un secret de Polichinelle, Red Bull l’...</td>\n      <td>course</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>https://www.eurosport.fr//formule-1/formule-1-...</td>\n      <td>Formule 1 - Rebondissement de dernière minute ...</td>\n      <td>FORMULE 1 - Selon les informations du quotidie...</td>\n      <td>https://imgresizer.eurosport.com/unsafe/1200x0...</td>\n      <td>Soirée mouvementée du côté du paddock. Alors q...</td>\n      <td>course</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>https://www.eurosport.fr//formule-1/frank-will...</td>\n      <td>Frank Williams hospitalisé</td>\n      <td>L'écurie Williams a fait savoir que Sir Frank ...</td>\n      <td>https://imgresizer.eurosport.com/unsafe/1200x0...</td>\n      <td>L'année 2020 n'est pas de tout repos pour la f...</td>\n      <td>course</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Spliting Training and validation data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize CamemBERT tokenizer\ntokenizer = CamembertTokenizer.from_pretrained('camembert-base',do_lower_case=True)","execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=810912.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ff250643097431da54d12c33cfb56bd"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creates list of texts and labels\n#text = df['text'].to_list()\n#labels = df['label'].to_list()\n\n# Creates list of texts and labels\ntext = df['text'].to_list()\nlabels = df['category_id'].to_list()\n\n#user tokenizer to convert sentences into tokenizer\ninput_ids  = [tokenizer.encode(sent,add_special_tokens=True,max_length=MAX_LEN) for sent in text]\n\n# Pad our input tokens\ninput_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n\n# Create attention masks\nattention_masks = []\n# Create a mask of 1s for each token followed by 0s for padding\nfor seq in input_ids:\n    seq_mask = [float(i>0) for i in seq]  \n    attention_masks.append(seq_mask)","execution_count":6,"outputs":[{"output_type":"stream","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = train_test_split(input_ids, labels, attention_masks,\n                                                            random_state=42, test_size=0.1)\n\ntrain_inputs = torch.tensor(train_inputs)\nvalidation_inputs = torch.tensor(validation_inputs)\ntrain_labels = torch.tensor(train_labels)\nvalidation_labels = torch.tensor(validation_labels)\ntrain_masks = torch.tensor(train_masks)\nvalidation_masks = torch.tensor(validation_masks)\n\n\n\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CamemBERT model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = CamembertForSequenceClassification.from_pretrained(\"camembert-base\", num_labels=18)#20\nmodel.to(device)","execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=508.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b49eb0039574d169f6a7cfcd4bde9a0"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=445032417.0, style=ProgressStyle(descri…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b86a7a40ced5490389de7bcbd29dfa46"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"stream","text":"Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","name":"stderr"},{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"CamembertForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=18, bias=True)\n  )\n)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Defining the parameters and metrics to optimize"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'gamma', 'beta']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.0}\n]\n\noptimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, eps=10e-8)#2e-5\n\n# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)","execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training and evaluating our model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Store our loss and accuracy for plotting if we want to visualize training evolution per epochs after the training process\ntrain_loss_set = []\n\n# trange is a tqdm wrapper around the normal python range\nfor _ in trange(epochs, desc=\"Epoch\"):  \n    # Tracking variables for training\n    tr_loss = 0\n    nb_tr_examples, nb_tr_steps = 0, 0\n  \n    # Train the model\n    model.train()\n    for step, batch in enumerate(train_dataloader):\n        # Add batch to device CPU or GPU\n        batch = tuple(t.to(device) for t in batch)\n        # Unpack the inputs from our dataloader\n        b_input_ids, b_input_mask, b_labels = batch\n        # Clear out the gradients (by default they accumulate)\n        optimizer.zero_grad()\n        # Forward pass\n        outputs = model(b_input_ids,token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n        # Get loss value\n        loss = outputs[0]\n        # Add it to train loss list\n        train_loss_set.append(loss.item())    \n        # Backward pass\n        loss.backward()\n        # Update parameters and take a step using the computed gradient\n        optimizer.step()\n    \n        # Update tracking variables\n        tr_loss += loss.item()\n        nb_tr_examples += b_input_ids.size(0)\n        nb_tr_steps += 1\n\n    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n    \n    \n\n\n    # Tracking variables for validation\n    eval_loss, eval_accuracy = 0, 0\n    nb_eval_steps, nb_eval_examples = 0, 0\n    # Validation of the model\n    model.eval()\n    # Evaluate data for one epoch\n    for batch in validation_dataloader:\n        # Add batch to device CPU or GPU\n        batch = tuple(t.to(device) for t in batch)\n        # Unpack the inputs from our dataloader\n        b_input_ids, b_input_mask, b_labels = batch\n        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n        with torch.no_grad():\n            # Forward pass, calculate logit predictions\n            outputs =  model(b_input_ids,token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n            loss, logits = outputs[:2]\n    \n        # Move logits and labels to CPU if GPU is used\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n\n        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n    \n        eval_accuracy += tmp_eval_accuracy\n        nb_eval_steps += 1\n\n    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))","execution_count":null,"outputs":[{"output_type":"stream","text":"\rEpoch:   0%|          | 0/3 [00:00<?, ?it/s]","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(model.state_dict(), f\"model_fr.bin\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nnews = [\"La Sud-Coréenne Kim A-lim a remporté sa première grande victoire internationale en réalisant un impressionnant retour dans le dernier tour de l\\'US Open féminin, lundi à Houston, au Texas.Kim A-lim, âgée de 25 ans, n'avait jamais gagné de tournoi en dehors du circuit sud-coréen. A Houston, elle a fini magnifiquement avec trois birdies pour s\\'offrir son premier majeur.Lors du dernier tour, elle a rendu une carte de 67, soit quatre sous le par, pour finir à -3 au score global du tournoi. Elle dépasse sa compatriote et numéro 1 mondiale Ko Jin-young et l'Américaine Amy Olson, qui complète le podium.Le 75e US Open féminin avait été reporté de juin à décembre en raison de la pandémie de coronavirus, et se déroulait à huis clos depuis jeudi, pour les mêmes raisons. Le 4e et dernier tour, prévu dimanche, a été repoussé à lundi en raison du risque d'orage sur le parcours du Champions Golf Club à Houston.\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encode the news\ntokenized_comments_ids = [tokenizer.encode(comment,add_special_tokens=True,max_length=MAX_LEN) for comment in comments]\n# Pad the resulted encoded news\ntokenized_comments_ids = pad_sequences(tokenized_comments_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n\n# Create attention masks \nattention_masks = []\nfor seq in tokenized_comments_ids:\n  seq_mask = [float(i>0) for i in seq]\n  attention_masks.append(seq_mask)\n\nprediction_inputs = torch.tensor(tokenized_comments_ids)\nprediction_masks = torch.tensor(attention_masks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply the finetuned model (Camembert)\nflat_pred = []\nwith torch.no_grad():\n    # Forward pass, calculate logit predictions\n    outputs =  model(prediction_inputs.to(device),token_type_ids=None, attention_mask=prediction_masks.to(device))\n    logits = outputs[0]\n    logits = logits.detach().cpu().numpy() \n    flat_pred.extend(np.argmax(logits, axis=1).flatten())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(flat_pred)):\n    print('Comment: ', news[i])\n    print('Label', flat_pred[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df['category_id'] == 8]","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}